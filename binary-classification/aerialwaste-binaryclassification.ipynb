{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8370534,"sourceType":"datasetVersion","datasetId":4976188}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Binary classification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!wget https://zenodo.org/records/7991872/files/testing.json -O testing.json\n!wget https://zenodo.org/records/7991872/files/training.json -O training.json","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:27:53.414348Z","iopub.execute_input":"2024-05-27T00:27:53.414695Z","iopub.status.idle":"2024-05-27T00:27:58.338716Z","shell.execute_reply.started":"2024-05-27T00:27:53.414668Z","shell.execute_reply":"2024-05-27T00:27:58.337175Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2024-05-27 00:27:54--  https://zenodo.org/records/7991872/files/testing.json\nResolving zenodo.org (zenodo.org)... 188.184.98.238, 188.185.79.172, 188.184.103.159, ...\nConnecting to zenodo.org (zenodo.org)|188.184.98.238|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1696233 (1.6M) [text/plain]\nSaving to: 'testing.json'\n\ntesting.json        100%[===================>]   1.62M  2.02MB/s    in 0.8s    \n\n2024-05-27 00:27:55 (2.02 MB/s) - 'testing.json' saved [1696233/1696233]\n\n--2024-05-27 00:27:56--  https://zenodo.org/records/7991872/files/training.json\nResolving zenodo.org (zenodo.org)... 188.184.98.238, 188.184.103.159, 188.185.79.172, ...\nConnecting to zenodo.org (zenodo.org)|188.184.98.238|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3142168 (3.0M) [text/plain]\nSaving to: 'training.json'\n\ntraining.json       100%[===================>]   3.00M  3.87MB/s    in 0.8s    \n\n2024-05-27 00:27:58 (3.87 MB/s) - 'training.json' saved [3142168/3142168]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -qU wandb\n!pip install -q torchsampler\n!pip install -qU torchmetrics","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:27:58.341061Z","iopub.execute_input":"2024-05-27T00:27:58.341407Z","iopub.status.idle":"2024-05-27T00:28:46.143957Z","shell.execute_reply.started":"2024-05-27T00:27:58.341373Z","shell.execute_reply":"2024-05-27T00:28:46.142520Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os, gc, sys, yaml, json, copy\nfrom pathlib import Path\nimport glob\nfrom collections import Counter, defaultdict\nfrom tqdm.auto import tqdm\n\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport PIL\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import model_selection\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import utils\nfrom torchvision import transforms as T\n\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nimport torchmetrics\nfrom torchsampler import ImbalancedDatasetSampler\n\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:28:46.145505Z","iopub.execute_input":"2024-05-27T00:28:46.145863Z","iopub.status.idle":"2024-05-27T00:28:57.465661Z","shell.execute_reply.started":"2024-05-27T00:28:46.145827Z","shell.execute_reply":"2024-05-27T00:28:57.464810Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:28:57.467962Z","iopub.execute_input":"2024-05-27T00:28:57.468470Z","iopub.status.idle":"2024-05-27T00:28:59.502305Z","shell.execute_reply.started":"2024-05-27T00:28:57.468442Z","shell.execute_reply":"2024-05-27T00:28:59.501269Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"CONFIG = dict(\n    img_size = [512, 512],\n    batch_size = 8,\n    epochs = 50,\n    seed = 42,\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n)\n\ndef seeding(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    torch.manual_seed(SEED)\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(SEED)\n        torch.cuda.manual_seed_all(SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n#     os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n#     tf.random.set_seed(SEED)\n#     keras.utils.set_random_seed(seed=SEED)\n    print('seeding done!!!')\n\ndef flush():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        \nseeding(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:28:59.503386Z","iopub.execute_input":"2024-05-27T00:28:59.503925Z","iopub.status.idle":"2024-05-27T00:28:59.536917Z","shell.execute_reply.started":"2024-05-27T00:28:59.503896Z","shell.execute_reply":"2024-05-27T00:28:59.535976Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"seeding done!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/aerial-dataset\")\nIMAGE_PATHS = glob.glob(\"/kaggle/input/aerial-dataset/*/*.png\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:28:59.537988Z","iopub.execute_input":"2024-05-27T00:28:59.538305Z","iopub.status.idle":"2024-05-27T00:29:03.035583Z","shell.execute_reply.started":"2024-05-27T00:28:59.538265Z","shell.execute_reply":"2024-05-27T00:29:03.034683Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"image_df = pd.DataFrame(IMAGE_PATHS, columns=['image_path'])\n\ncheck_path = lambda path: os.path.exists(path)\nget_image_dir = lambda path: int(Path(path).stem)\nimage_df['image_id'] = image_df['image_path'].map(get_image_dir)\nimage_df['exists'] = image_df['image_path'].map(check_path)\nimage_df['exists'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:03.036635Z","iopub.execute_input":"2024-05-27T00:29:03.036906Z","iopub.status.idle":"2024-05-27T00:29:17.163215Z","shell.execute_reply.started":"2024-05-27T00:29:03.036883Z","shell.execute_reply":"2024-05-27T00:29:17.162256Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"exists\nTrue    10977\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"with open(\"/kaggle/working/training.json\", \"r\") as f:\n    data = json.loads(f.read())","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.164628Z","iopub.execute_input":"2024-05-27T00:29:17.165148Z","iopub.status.idle":"2024-05-27T00:29:17.220415Z","shell.execute_reply.started":"2024-05-27T00:29:17.165080Z","shell.execute_reply":"2024-05-27T00:29:17.219622Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = pd.json_normalize(data, record_path=['images'])\ndf.rename(columns={'id': 'image_id'}, inplace=True)\ndf = df.merge(image_df, how='left', on='image_id')\ndf.rename(columns={'is_candidate_location': 'label'}, inplace=True)\n\n# CONFIG = len(df['label'].unique())","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.221695Z","iopub.execute_input":"2024-05-27T00:29:17.222427Z","iopub.status.idle":"2024-05-27T00:29:17.806122Z","shell.execute_reply.started":"2024-05-27T00:29:17.222389Z","shell.execute_reply":"2024-05-27T00:29:17.804898Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# # Assuming 'df' is your DataFrame and 'is_candidate_location' is the column of interest\n# data = df['label'].value_counts()\n\n# # Calculate percentages\n# percentages = data / data.sum() * 100\n\n# # Define the separation of sectors\n# explode = [0.1] * len(data)  # This will separate all sectors slightly. Adjust as needed.\n\n# # Create the pie chart\n# plt.figure(figsize=(10, 5))\n# plt.pie(data, labels=data.index, autopct='%1.1f%%', startangle=140, explode=explode)\n\n# # Equal aspect ratio ensures that pie is drawn as a circle.\n# plt.axis('equal')  \n# plt.title(\"Distribution of Landfill Candidate Location in Data\")\n\n# # Save the figure\n# # plt.savefig(\"landfill_distribution_pie.png\")\n\n# # Show the plot\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.811456Z","iopub.execute_input":"2024-05-27T00:29:17.811889Z","iopub.status.idle":"2024-05-27T00:29:17.817087Z","shell.execute_reply.started":"2024-05-27T00:29:17.811860Z","shell.execute_reply":"2024-05-27T00:29:17.815818Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class AerialDataset(utils.data.Dataset):\n    \n    def __init__(self, data, transform, mode='train'):\n        super().__init__()\n        self.data = data\n        self.tsfm = transform\n        self.mode = mode\n        self.label = data.loc[:, 'label']\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_path = self.data.loc[idx, 'image_path']\n        target = self.data.loc[idx, 'label']\n        \n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.tsfm(image=image)['image']\n        \n        return {\"image\": image, \n                \"target\": torch.tensor(target, dtype=torch.float)\n               }\n    \n    def get_labels(self):\n        return self.label","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.818468Z","iopub.execute_input":"2024-05-27T00:29:17.818864Z","iopub.status.idle":"2024-05-27T00:29:17.830579Z","shell.execute_reply.started":"2024-05-27T00:29:17.818829Z","shell.execute_reply":"2024-05-27T00:29:17.829543Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"ts = A.Compose([\n    A.Resize(height=512, width=512),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\nds = AerialDataset(data=df, transform=ts)\ndls = utils.data.DataLoader(ds, batch_size=8, shuffle=True, num_workers=os.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.831950Z","iopub.execute_input":"2024-05-27T00:29:17.832559Z","iopub.status.idle":"2024-05-27T00:29:17.842129Z","shell.execute_reply.started":"2024-05-27T00:29:17.832525Z","shell.execute_reply":"2024-05-27T00:29:17.841112Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# def get_transforms(height, width):\n#     train_tsfm = A.Compose([\n#         A.Resize(height=height, width=width),\n# #         A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n# #         A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n        \n# #         A.OneOf([\n# #             A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=5, p=1),\n# #             A.RandomSnow(brightness_coeff=2.5, snow_point_lower=0.3, snow_point_upper=0.5, p=1),\n# #             A.RandomFog(fog_coef_lower=0.7, fog_coef_upper=0.8, alpha_coef=0.1, p=1),\n# #         ], p=0.3),\n        \n# #         A.OneOf([\n# #             A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0.5, p=1),\n# #             A.RandomShadow(num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=1),\n# #         ], p=0.2),\n        \n#         A.Normalize(mean=[0.485, 0.456, 0.406],\n#                    std=[0.229, 0.224, 0.225]),\n#         ToTensorV2()\n#     ])\n    \n#     valid_tsfm = A.Compose([\n#         A.Resize(height=height, width=width),\n#         A.Normalize(mean=[0.485, 0.456, 0.406],\n#                    std=[0.229, 0.224, 0.225]),\n#         ToTensorV2()\n#     ])\n#     return {\"train\": train_tsfm, \"eval\": valid_tsfm}\n\n\ndef get_transforms(height, width):\n    train_tsfm = A.Compose([\n        # Geometric augmentations\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.5),\n        # Photometric augmentations\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n        A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n        A.Resize(height=height, width=width),\n        # Normalization and conversion to tensor\n        A.Normalize(mean=[0.485, 0.456, 0.406], \n                    std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    \n    valid_tsfm = A.Compose([\n        A.Resize(height=height, width=width),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    \n    return {\"train\": train_tsfm, \"eval\": valid_tsfm}\n\n\ndef get_dataloaders(data, cfg, split=\"train\"):\n    img_size = cfg['img_size']\n    height, width = img_size[0], img_size[1]\n    tsfm = get_transforms(height=height, width=width)\n    if split == 'train':\n        tr_tsfm = tsfm['train']\n        ds = AerialDataset(data=data, transform=tr_tsfm)\n        labels = ds.get_labels()\n        class_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels))\n        samples_weights = class_weights[labels]\n        sampler = utils.data.WeightedRandomSampler(weights=samples_weights, \n                                                   num_samples=len(samples_weights), \n                                                   replacement=True)\n#         sampler=ImbalancedDatasetSampler(ds)\n        dls = utils.data.DataLoader(ds, \n                                    batch_size=cfg['batch_size'], \n                                    sampler=sampler,\n                                    num_workers=os.cpu_count(), \n                                    drop_last=True, \n                                    pin_memory=True)\n        \n    elif split == 'valid' or split == 'test':\n        eval_tsfm = tsfm['eval']\n        ds = AerialDataset(data=data, transform=eval_tsfm)\n        dls = utils.data.DataLoader(ds, \n                                    batch_size=2*cfg['batch_size'], \n                                    shuffle=False, \n                                    num_workers=os.cpu_count(), \n                                    drop_last=False, \n                                    pin_memory=True)\n    else:\n        raise Exception(\"Split should be 'train' or 'valid' or 'test'!!!\")\n    return dls","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.843670Z","iopub.execute_input":"2024-05-27T00:29:17.844299Z","iopub.status.idle":"2024-05-27T00:29:17.867984Z","shell.execute_reply.started":"2024-05-27T00:29:17.844264Z","shell.execute_reply":"2024-05-27T00:29:17.866690Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def check_class_distribution(data_loader):\n    for i, batch in enumerate(data_loader):\n        labels = batch['target']\n        class_distribution = Counter(labels.numpy())\n        print(f\"Batch {i+1}: Class Distribution: {class_distribution}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.869417Z","iopub.execute_input":"2024-05-27T00:29:17.874171Z","iopub.status.idle":"2024-05-27T00:29:17.879985Z","shell.execute_reply.started":"2024-05-27T00:29:17.874128Z","shell.execute_reply":"2024-05-27T00:29:17.878747Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# kfold = model_selacosection.GroupKFold(n_splits=5)\nkfold = model_selection.StratifiedKFold(n_splits=4, shuffle=True, random_state=2024)\nx = df.index.values\ny = df['label'].astype(int).values\n\ndf['fold'] = -1\nfor fold, (tr_idx, val_idx) in enumerate(kfold.split(x,y)):\n    df.loc[val_idx, 'fold'] = fold","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.881682Z","iopub.execute_input":"2024-05-27T00:29:17.882125Z","iopub.status.idle":"2024-05-27T00:29:17.902741Z","shell.execute_reply.started":"2024-05-27T00:29:17.882047Z","shell.execute_reply":"2024-05-27T00:29:17.901842Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Feature Pyramid Network","metadata":{}},{"cell_type":"code","source":"def gap2d(x, keepdims=False):\n    out = torch.mean(x.view(x.size(0), x.size(1), -1), -1)\n    if keepdims:\n        out = out.view(out.size(0), out.size(1), 1, 1)\n\n    return out\n\n\nclass ResnetFPN(nn.Module):\n    def __init__(self, name, num_classes, pretrained=False, first_trainable=0):\n        super(ResnetFPN, self).__init__()\n        self.num_classes = num_classes\n        self.first_trainable = first_trainable\n        self.encoder = timm.create_model(name, pretrained=pretrained, features_only=True)\n        \n        # first backbone layers\n        self.stage0 = nn.Sequential(self.encoder.conv1,\n                                   self.encoder.bn1,\n                                   self.encoder.act1,\n                                   self.encoder.maxpool)\n        \n        # Backbone layers (bottom-up layer)\n        self.stage1 = nn.Sequential(self.encoder.layer1)\n        self.stage2 = nn.Sequential(self.encoder.layer2)\n        self.stage3 = nn.Sequential(self.encoder.layer3)\n        self.stage4 = nn.Sequential(self.encoder.layer4)\n        \n        if 'resnet18' in name.lower() or 'resnet34' in name.lower():\n            in_chans = self.encoder.layer4[-1].conv2.out_channels\n        if 'resnet50' in name.lower():\n            in_chans = self.encoder.layer4[-1].conv3.out_channels\n        \n        out_chans = in_chans // 8\n        # Top Layer\n        self.toplayer = nn.Conv2d(\n            in_chans, out_chans, kernel_size=1, stride=1, padding=0)\n\n        # Lateral Layers\n        self.latlayer1 = nn.Conv2d(\n            in_chans // 2, out_chans, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d(\n            in_chans // 4, out_chans, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d(\n            out_chans, out_chans, kernel_size=1, stride=1, padding=0)\n        \n        # smooth layers\n        mid_chans = in_chans // 2 - out_chans\n        self.smooth1 = nn.Conv2d(in_chans // 4, out_chans, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(mid_chans, out_chans, kernel_size=3, stride=1, padding=1)\n        self.smooth3 = nn.Conv2d(in_chans // 2, out_chans, kernel_size=3, stride=1, padding=1)\n        \n        # fully connected layer\n        self.fc = nn.Linear(out_chans, num_classes)\n        \n        # last fully connected layer\n        self.classifier = nn.Linear(4*num_classes, num_classes)\n        \n        self.backbone = nn.ModuleList(\n            [self.stage0, self.stage1, self.stage2, self.stage3, self.stage4]\n        )\n        \n        self.newly_added = nn.ModuleList(\n            [self.toplayer, self.latlayer1, self.latlayer2, self.latlayer3, self.smooth1, self.smooth2, self.smooth3,\n            self.fc, self.classifier]\n        )\n        \n    \n    def forward(self, x):\n        # bottom-up pathway \n        c1 = self.stage0(x)\n        c2 = self.stage1(c1)\n        c3 = self.stage2(c2).detach()\n        c4 = self.stage3(c3)\n        c5 = self.stage4(c4)\n        \n        # top-down pathway\n        p5 = self.toplayer(c5)\n        p4 = self._upsample_cat(p5, self.latlayer1(c4))\n        p3 = self._upsample_cat(p4, self.latlayer2(c3))\n        p2 = self._upsample_cat(p3, self.latlayer3(c2))\n        \n        # smoothing (de-aliasing effect)\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        p2 = self.smooth3(p2)\n        \n        # Global Average Pooling\n        p5 = gap2d(p5, keepdims=True)\n        p4 = gap2d(p4, keepdims=True)\n        p3 = gap2d(p3, keepdims=True)\n        p2 = gap2d(p2, keepdims=True)\n        \n        # Flattening\n        p5 = p5.view(p5.size(0), -1)\n        p4 = p4.view(p4.size(0), -1)\n        p3 = p3.view(p3.size(0), -1)\n        p2 = p2.view(p2.size(0), -1)\n        \n        # Fully connected layers\n        out5 = F.relu(self.fc(p5))\n        out4 = F.relu(self.fc(p4))\n        out3 = F.relu(self.fc(p3))\n        out2 = F.relu(self.fc(p2))\n        \n        # concatenate the predictions (classification results) of each of the pyramid features\n        out = torch.cat([out5, out4, out3, out2], dim=1)\n        out = self.classifier(out)\n        return out\n    \n    def _upsample_cat(self, x, y):\n        _, _, H, W = y.size()\n        upsampled_x = F.interpolate(\n            x, size=(H,W), mode=\"nearest\"\n        )\n        return torch.cat([upsampled_x, y], dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.904343Z","iopub.execute_input":"2024-05-27T00:29:17.904729Z","iopub.status.idle":"2024-05-27T00:29:17.933021Z","shell.execute_reply.started":"2024-05-27T00:29:17.904694Z","shell.execute_reply":"2024-05-27T00:29:17.932018Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.934078Z","iopub.execute_input":"2024-05-27T00:29:17.934395Z","iopub.status.idle":"2024-05-27T00:29:17.946639Z","shell.execute_reply.started":"2024-05-27T00:29:17.934370Z","shell.execute_reply":"2024-05-27T00:29:17.945658Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# EfficientNet","metadata":{}},{"cell_type":"code","source":"class AerialModel(nn.Module):\n    def __init__(self, \n                 name: str, \n                 num_classes: int = 1, \n                 pretrained: bool = False, \n                 kernel_size: int = 3, \n                 stride: int = 2):\n        \n        super().__init__()\n        self.encoder = timm.create_model(name, pretrained=pretrained, num_classes=0)\n        nb_fts = self.encoder.num_features\n        nb_fts = nb_fts // stride\n        self.nb_fts = nb_fts if kernel_size < 3 else nb_fts - 1\n        self.avg_pool = nn.AvgPool1d(kernel_size, stride=stride)\n        \n        self.flatten = nn.Flatten()\n        self.head = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.nb_fts, 768),\n            nn.ReLU(),\n#             nn.BatchNorm1d(768),\n            nn.Dropout(0.2),\n            nn.Linear(768, 64),\n            nn.ReLU(),\n#             nn.BatchNorm1d(256),\n            nn.Dropout(0.2),\n            nn.Linear(64, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.flatten(x)\n        x = self.avg_pool(x)\n        \n        outputs = self.head(x)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.948011Z","iopub.execute_input":"2024-05-27T00:29:17.948654Z","iopub.status.idle":"2024-05-27T00:29:17.958567Z","shell.execute_reply.started":"2024-05-27T00:29:17.948620Z","shell.execute_reply":"2024-05-27T00:29:17.957654Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def shared_step(batch, criterion, params):\n    image, target = batch[\"image\"], batch[\"target\"]\n    image = image.to(params[\"device\"], non_blocking=True)\n    target = target.to(params[\"device\"], non_blocking=True)\n\n    logits = model.forward(image.to(torch.float32))\n    loss = criterion(logits.squeeze(), target)\n    \n    preds = logits.sigmoid().squeeze()\n    \n    return {\n        \"loss\": loss,\n        \"preds\": preds\n    }\n    \n    \ndef train(train_loader, model, criterion, optimizer, epoch, scaler, params):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    train_loss = 0\n    for i, batch in enumerate(stream, start=1):\n        optimizer.zero_grad(set_to_none=True)\n        \n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = shared_step(batch, criterion, params)\n            loss =  outputs['loss']\n        \n#         accuracy = METRICS['accuracy'](predictions, target)\n#         jaccard = METRICS['jaccard_index'](predictions, target)\n#         fbeta = METRICS['fbeta_score'](predictions, target)\n        \n        metric_monitor.update(\"Loss\", loss.item())\n#         metric_monitor.update(\"Accuracy\", accuracy)\n#         metric_monitor.update(\"Jaccard\", jaccard)\n#         metric_monitor.update(\"FBeta\", fbeta)\n        \n        train_loss += loss.detach().float()\n        lr = optimizer.param_groups[0]['lr']\n        _train_metrics = {\n            \"train/step_loss\": loss.item(),\n            \"learning_rate\": lr,\n#             \"train/accuracy\": accuracy,\n#             \"train/jaccard_index\": jaccard,\n#             \"train/fbeta_score\": fbeta\n        }\n        \n#         wandb.log({})\n        if (i+1) % 50 == 0:\n            wandb.log(_train_metrics)\n            \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n        )\n        \n    total_train_loss = train_loss / len(train_loader)\n    \n    flush()\n    return _train_metrics, total_train_loss\n        \ndef validate(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    valid_loss = 0\n    \n    with torch.no_grad():\n        for i, batch in enumerate(stream, start=1):\n            outputs = shared_step(batch, criterion, params)\n            loss =  outputs['loss']\n            \n#             accuracy = METRICS['accuracy'](predictions, target)\n#             jaccard = METRICS['jaccard_index'](predictions, target)\n#             fbeta = METRICS['fbeta_score'](predictions, target)\n\n            metric_monitor.update(\"Loss\", loss.item())\n#             metric_monitor.update(\"Accuracy\", accuracy)\n#             metric_monitor.update(\"Jaccard\", jaccard)\n#             metric_monitor.update(\"FBeta\", fbeta)\n            \n            valid_loss += loss.detach().float()\n            _valid_metrics = {\n                \"valid/step_loss\": loss.item(),\n#                 \"valid/loss\": valid_loss,\n#                 \"valid/accuracy\": accuracy,\n#                 \"valid/jaccard_index\": jaccard,\n#                 \"valid/fbeta_score\": fbeta\n            }\n\n            if (i+1) % 10 == 0:\n                wandb.log(_valid_metrics)\n            \n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n            )\n            \n    flush()\n    total_valid_loss = valid_loss / len(val_loader)\n    return _valid_metrics, total_valid_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.959986Z","iopub.execute_input":"2024-05-27T00:29:17.960285Z","iopub.status.idle":"2024-05-27T00:29:17.977548Z","shell.execute_reply.started":"2024-05-27T00:29:17.960261Z","shell.execute_reply":"2024-05-27T00:29:17.976601Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train_and_validate(model, train_dataset, val_dataset, params, fold=0):\n    model = model.to(params['device'])\n    run = wandb.init(\n        project=\"PlasticOpticsBinaryClassification\",\n        resume=\"allow\"\n    )\n    \n    artifact = wandb.Artifact(f\"aerialBiFPNModel_fold_{fold}\", type=\"model\")\n    train_loader = get_dataloaders(train_data, cfg=CONFIG, split='train')\n    val_loader = get_dataloaders(valid_data, cfg=CONFIG, split=\"valid\")\n    \n    criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n    scaler = torch.cuda.amp.GradScaler()\n    \n#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n#                                                                      params['epochs'], \n#                                                                      eta_min=0)\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['epochs'], eta_min=0)\n    lr_reduce = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4, verbose=True)\n    \n    best_metric = np.inf\n    loss_min = np.inf\n    es = 0\n    ES_RATIO = 0.25 if params['epochs'] < 10 else 0.20\n#     seg_weights_file = \"aerialwaste_binary_fpn_weights.pth\"\n    weights_file = \"aerialwaste_binary_classification_fold_{fold}_epoch_{epoch}.pth\"\n    for epoch in range(1, params[\"epochs\"] + 1):\n        scheduler.step()\n        _train_metrics, train_loss = train(train_loader, model, criterion, optimizer, epoch, scaler, params)\n        _valid_metrics, val_loss = validate(val_loader, model, criterion, epoch, params)\n        \n        _train_metrics[\"train/loss\"] = train_loss\n        _valid_metrics[\"valid/loss\"] = val_loss\n        lr_reduce.step(val_loss)\n        wandb.log({**_train_metrics, **_valid_metrics})\n        if val_loss < best_metric:\n            print(f\"Best metric: ({best_metric:.6f} --> {val_loss:.6f}). Saving model ...\")\n#             torch.save(model.module.state_dict(), f\"{name}_fold_{fold}.pth\")\n            weights_file.format(fold=fold, epoch=epoch)\n            torch.save(model.state_dict(), weights_file)\n            best_metric = val_loss\n            es = 0\n            if epoch == 1:\n                artifact.add_file(weights_file)\n                run.log_artifact(artifact)\n            else:\n                draft_artifact = wandb.Artifact(f\"aerialMultiFPNModel_fold_{fold}\", type=\"model\")\n                draft_artifact.add_file(weights_file)\n                run.log_artifact(draft_artifact)\n                \n        else:\n            es += 1\n            \n        if es > math.ceil(ES_RATIO*params['epochs']):\n            print(f\"Early stopping on epoch {epoch} ...\")\n            break\n            \n    wandb.config = params\n    wandb.finish()\n    flush()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.978827Z","iopub.execute_input":"2024-05-27T00:29:17.979162Z","iopub.status.idle":"2024-05-27T00:29:17.994002Z","shell.execute_reply.started":"2024-05-27T00:29:17.979136Z","shell.execute_reply":"2024-05-27T00:29:17.993025Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"CONFIG['lr'] = 5e-3\n\nfor fold in range(4):\n#     model = ResnetFPN(name='resnet34', num_classes=1, pretrained=True)\n    model = AerialModel(name='efficientnet_b0.ra_in1k', pretrained=True)\n    train_data = df[df['fold'] != fold].reset_index(drop=True)\n    valid_data = df[df['fold'] == fold].reset_index(drop=True)\n    train_and_validate(model, train_data, valid_data, params=CONFIG, fold=fold)\n    \ngc.collect()\nflush()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:29:17.995269Z","iopub.execute_input":"2024-05-27T00:29:17.995554Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3901c256c1b946a7a851d39c9d1a1b89"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamu2505\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240527_002920-g19b2kap</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/g19b2kap' target=\"_blank\">restful-morning-14</a></strong> to <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification' target=\"_blank\">https://wandb.ai/samu2505/PlasticOpticsBinaryClassification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/g19b2kap' target=\"_blank\">https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/g19b2kap</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f4ed098c6074fa2aea42c86a3e6a931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519aaaf255714e07b3a31a07023b3b39"}},"metadata":{}},{"name":"stdout","text":"Best metric: (inf --> 0.530150). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"886db0d98c2149fba11ca5974809f8e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a25f80fc42f34e5694f7598285a269cc"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.530150 --> 0.457684). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dab3874197c4617947d831132439632"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5718224a5f854ca7a58d6b4fe003d9c4"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.457684 --> 0.434909). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ca4f6c096346c296fa5059611df03f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0cf95a1708449afbffb9dfe22135255"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.434909 --> 0.416211). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d15bd418dd4f8a97cbb3bb1be50a00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ea2bfb1d40643a9bdab475c24723766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d728e0bb3840ba96da59be9401c90f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1bde10ae1db4192bec6ce9a590c6d55"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.416211 --> 0.413689). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e45a0033d8c648dc826b0bbd5a442677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d1114f764c43abb6cfbff842d0f564"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.413689 --> 0.405633). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07556de283a6466a815111e432622e6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ed394bd7064b818f4ffeda68352dbf"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.405633 --> 0.365699). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6bcbff8d874d2fae94a216cb63a5fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee6960201a344e9889c7415aa468d50"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.365699 --> 0.351014). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634177b0c0f14446997fad80a820b28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bda118447a241d284ba6c92ea8121cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d51ffd051e754c1a87a0fa92b87c045f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e569341ad41495cb6ecfeabcf046eb6"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.351014 --> 0.344930). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e0641f42f04935ad310904b6333e1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107cb4619453427996948e985907f685"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.344930 --> 0.312490). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02e52c6e7134621a9ee633f6506ff4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3acb099d5614e1da081c363a5daa123"}},"metadata":{}},{"name":"stdout","text":"Best metric: (0.312490 --> 0.309152). Saving model ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b9707cf39f4cde8d40bf154928d39a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e3032844374a419da10fe192fb45b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c448f45a2b46f99cd5a90faa50e591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8795db9b82da4decb25afb13f3cc4083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='194.232 MB of 194.232 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>██████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▅▄▄▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/step_loss</td><td>▅▄▄▃▅▂█▅▅▃▄▄▄▄▃▃▂▁▂▃▁▅▁▃▂▁▄▃▃▂▂▁▂▁▂▂▂▁▂▁</td></tr><tr><td>valid/loss</td><td>█▆▅▄▆▄▄▃▂▄▂▁▁▁▁</td></tr><tr><td>valid/step_loss</td><td>▅▅▅▄▅█▂▃▆▄▂▄▂▇▃▃▄▅▇▅▂▅▂▁▂▅▇▃▂▄▃▁▃▁▅▃▁▂▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3541</td></tr><tr><td>train/step_loss</td><td>0.15652</td></tr><tr><td>valid/loss</td><td>0.31374</td></tr><tr><td>valid/step_loss</td><td>0.45446</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">restful-morning-14</strong> at: <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/g19b2kap' target=\"_blank\">https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/g19b2kap</a><br/> View project at: <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification' target=\"_blank\">https://wandb.ai/samu2505/PlasticOpticsBinaryClassification</a><br/>Synced 5 W&B file(s), 0 media file(s), 11 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240527_002920-g19b2kap/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240527_022721-hc0kuhwe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/hc0kuhwe' target=\"_blank\">whole-dust-15</a></strong> to <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification' target=\"_blank\">https://wandb.ai/samu2505/PlasticOpticsBinaryClassification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/hc0kuhwe' target=\"_blank\">https://wandb.ai/samu2505/PlasticOpticsBinaryClassification/runs/hc0kuhwe</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/784 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5354ed647b4c3b8d70a3dd6c825704"}},"metadata":{}}]},{"cell_type":"code","source":"# batch = next(iter(dls))\n# encoder = timm.create_model('tf_efficientnet_b7.ra_in1k', pretrained=False, features_only=True)\n# encoder = timm.create_model('resnet18', pretrained=False, features_only=True)\n# encoder = timm.create_model('resnet18', pretrained=False, features_only=True)\n# model = ResnetFPN(name='resnet34', num_classes=1)\n# model.eval()\n# out = model(batch['image'])\n# out = encoder(batch['image'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class FPNModel(pl.LightningModule):\n#     def __init__(self, name, num_classes, pretrained=False, lr=1e-3):\n#         super().__init__()\n#         self.save_hyperparameters()\n#         self.model = ResnetFPN(name=name, num_classes=num_classes, pretrained=pretrained)\n#         self.loss_fn = nn.BCEWithLogitsLoss()\n#     def forward(self, x):\n#         return self.model(x)\n    \n#     def training_step(self, batch, batch_idx):\n#         imgs, labels = batch['image'], batch['target']\n#         preds = self.forward(imgs)\n#         loss = self.loss_fn(preds.squeeze(), labels)\n#         self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n#         return loss\n    \n#     def configure_optimizers(self):\n#         max_epochs = self.trainer.max_epochs\n#         optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n# #         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n# #         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=0)\n        \n#         scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n#             optimizer, T_0=10, T_mult=2)\n        \n# #         scheduler = torch.optim.lr_scheduler.OneCycleLR(\n# #             optimizer=optimizer, epochs=max_epochs,\n# #             pct_start=0.0, steps_per_epoch=self.steps_per_epoch,\n# #             max_lr=self.hparams.lr, div_factor=25, final_div_factor=4.0e-01\n# #         )\n        \n#         return [optimizer], [scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# net = FPNModel(name='resnet18', num_classes=1)\n# trainer = pl.Trainer(max_epochs=10)\n# trainer.fit(net, train_dataloaders=dls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(net.state_dict(), 'aerial_binary.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross entropy loss","metadata":{}},{"cell_type":"code","source":"# class FocalLoss(nn.Module):\n#     def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n#         super(FocalLoss, self).__init__()\n#         self.alpha = alpha\n#         self.gamma = gamma\n#         self.reduction = reduction\n\n#     def forward(self, inputs, targets):\n#         BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n#         pt = torch.exp(-BCE_loss)  # prevents nans when probability 0\n#         F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n#         if self.reduction == 'mean':\n#             return torch.mean(F_loss)\n#         elif self.reduction == 'sum':\n#             return torch.sum(F_loss)\n#         else:\n#             return F_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y = df['label'].astype(int).values\n# # neg_examples, pos_examples = np.bincount(df['label'].astype(int).values)\n# # pos_weights = torch.tensor([neg_examples / pos_examples])\n# # pos_weights\n# pos_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y))\n# # np.unique(df['label'].astype(int).values, return_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class AerialLightningModel(pl.LightningModule):\n#     def __init__(self, \n#                  name: str, \n#                  num_classes: int = 0, \n#                  pretrained: bool = False, \n#                  kernel_size: int = 3, \n#                  stride: int = 2, \n#                  lr: float = 3e-4):\n        \n#         super().__init__()\n#         self.save_hyperparameters()\n#         self.encoder = timm.create_model(name, pretrained=pretrained, num_classes=0)\n#         nb_fts = self.encoder.num_features\n#         nb_fts = nb_fts // stride\n#         self.nb_fts = nb_fts if kernel_size < 3 else nb_fts - 1\n#         self.avg_pool = nn.AvgPool1d(kernel_size, stride=stride)\n        \n#         self.flatten = nn.Flatten()\n#         self.head = nn.Sequential(\n#             nn.Dropout(0.2),\n#             nn.Linear(self.nb_fts, 768),\n#             nn.ReLU(),\n# #             nn.BatchNorm1d(768),\n#             nn.Dropout(0.2),\n#             nn.Linear(768, 256),\n#             nn.ReLU(),\n# #             nn.BatchNorm1d(256),\n#             nn.Dropout(0.2),\n#             nn.Linear(256, num_classes)\n#         )\n        \n#         self.loss_fn = nn.BCEWithLogitsLoss()\n#         self.accuracy = torchmetrics.Accuracy(task='binary')\n#         self.recall = torchmetrics.Recall(task='binary')\n#         self.precision = torchmetrics.Precision(task='binary')\n#         self.step_outputs = []\n        \n#     def forward(self, x):\n#         x = self.encoder(x)\n#         x = self.flatten(x)\n#         x = self.avg_pool(x)\n        \n#         outputs = self.head(x)\n#         return outputs\n    \n#     def freeze_encoder(self, flag):\n#         for param in self.encoder.parameters():\n#             param.requires_grad = not flag\n    \n#     def shared_step(self, batch, stage):\n#         imgs, labels = batch['image'], batch['target']\n#         preds = self.forward(imgs)\n#         loss = self.loss_fn(preds.squeeze(), labels)\n# #         preds = (preds.sigmoid().squeeze() > 0.5).float()\n#         preds = preds.sigmoid().squeeze()\n#         acc = self.accuracy(preds, labels)\n#         recall = self.recall(preds, labels)\n#         precision = self.precision(preds, labels)\n        \n#         self.log(f'{stage}_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n#         self.log(f'{stage}_acc', acc, on_step=False, on_epoch=True, prog_bar=False)\n#         self.log(f'{stage}_recall', recall, on_step=False, on_epoch=True, prog_bar=False)\n#         self.log(f\"{stage}_precision\", precision, on_step=False, on_epoch=True, prog_bar=False)\n        \n#         output = {\n#             f\"{stage}_loss\": loss,\n#             f\"{stage}_acc\": acc,\n#             f\"{stage}_recall\": recall,\n#             f\"{stage}_precision\": precision,\n#             f\"{stage}_labels\": labels,\n#             f\"{stage}_preds\": preds\n#         }\n#         self.step_outputs.append(output)\n#         return output\n    \n#     def training_step(self, batch, batch_idx):\n#         output = self.shared_step(batch, 'train')\n#         loss = output['train_loss']\n#         return loss\n    \n#     def validation_step(self, batch, batch_idx):\n# #         output = self.shared_step(batch, 'val')\n# #         labels, preds = output['val_labels'], output['val_preds']\n# #         if batch_idx % 10 == 1:\n# #             sys.stdout.write('\\033[F'*n)\n# #             sys.stdout.write('\\033[K')\n# #             print(f\"Batch idx: {batch_idx} -> Labels: {labels}, Preds: {preds}\")\n# #             sys.stdout.flush()\n            \n# #         clear_output(wait=True)\n#         return self.shared_step(batch, 'val')\n            \n\n#     def test_step(self, batch, batch_idx):\n#         return self.shared_step(batch, 'test')\n\n#     def predict_step(self, batch, batch_idx, dataloader_idx=0):\n#         return self(batch)\n    \n#     def configure_optimizers(self):\n#         max_epochs = self.trainer.max_epochs\n#         optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n# #         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n# #         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=0)\n        \n#         scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n#             optimizer, T_0=10, T_mult=2)\n        \n# #         scheduler = torch.optim.lr_scheduler.OneCycleLR(\n# #             optimizer=optimizer, epochs=max_epochs,\n# #             pct_start=0.0, steps_per_epoch=self.steps_per_epoch,\n# #             max_lr=self.hparams.lr, div_factor=25, final_div_factor=4.0e-01\n# #         )\n        \n#         return [optimizer], [scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CONFIG['patience'] = 8 if CONFIG['epochs'] < 50 else 12\n# for fold in range(4):\n#     train_data = df[df['fold'] != fold].reset_index(drop=True)\n#     valid_data = df[df['fold'] == fold].reset_index(drop=True)\n#     train_dls = get_dataloaders(train_data, cfg=CONFIG, split='train')\n#     valid_dls = get_dataloaders(valid_data, cfg=CONFIG, split=\"valid\")\n\n#     net = AerialLightningModel(name='efficientnet_b0.ra_in1k', \n#                                num_classes=1, \n#                                pretrained=True, \n#                                lr=1e-5)\n#     # net.freeze_encoder(True)\n\n#     wandb_logger = WandbLogger(project=\"Plastic-Optics-classification\",\n#                                checkpoint_name=f\"aerialBinary_fold_{fold}\", \n#                                log_model=\"all\")\n\n#     callbacks = [\n#         ModelCheckpoint(save_weights_only=True, \n#                         mode=\"min\", \n#                         monitor=\"val_loss\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n#         LearningRateMonitor(\"epoch\"),\n#         EarlyStopping(monitor=\"val_loss\", min_delta=0.0, patience=CONFIG['patience'], verbose=False, mode=\"min\"),\n#     ]\n\n#     trainer = pl.Trainer(max_epochs=CONFIG['epochs'], logger=wandb_logger, callbacks=callbacks)\n# #     trainer = pl.Trainer(max_epochs=10)\n#     trainer.fit(net, train_dataloaders=train_dls, val_dataloaders=valid_dls)\n#     break\n# wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# help(torchmetrics.Precision(task='binary', num_classes=2))\n# help(torchmetrics.Recall(task='binary'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}